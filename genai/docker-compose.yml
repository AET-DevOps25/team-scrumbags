services:

  app:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - WEAVIATE_URL=http://weaviate:8080
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq/
    volumes:
      - .:/app

  rabbitmq:
    image: rabbitmq:management
    container_name: rabbitmq
    restart: always
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    volumes:
      - rabbitmq-lib:/var/lib/rabbitmq/
      - rabbitmq-log:/var/log/rabbitmq

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8080'
      - --scheme
      - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.31.2
    ports:
      - "8080:8080"
      - "50051:50051"
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_API_BASED_MODULES: 'true'
      CLUSTER_HOSTNAME: 'node1'

  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    # override the container start command
    entrypoint:
      - sh
      - -c
      - |
        set -e

        echo "▶️  Starting Ollama server…"
        # Launch server in background
        ollama serve &
        SERVER_PID=$!

        echo "⏳  Waiting for server to be healthy…"
        # simple health‑check loop: try 'ollama list' until it returns success
        until ollama list > /dev/null 2>&1; do
          sleep 1
        done

        echo "✅  Server is up! Pulling llama3.2…"
        # Now that the daemon is running, pull the model
        ollama pull llama3.2

        echo "🚀  Model pulled. Waiting on server (PID $SERVER_PID)…"
        # Drop into the Ollama server process so Docker keeps the container alive
        wait $SERVER_PID
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: always
    healthcheck:
      test: [ "CMD-SHELL", "ollama list | grep -q llama3.2" ]
      interval: 30s
      timeout: 10s
      retries: 10


volumes:
  weaviate_data:
  rabbitmq-lib:
    driver: local
  rabbitmq-log:
    driver: local
  ollama: